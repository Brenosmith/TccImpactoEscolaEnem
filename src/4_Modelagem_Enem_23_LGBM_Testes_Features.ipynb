{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d08606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV  # Bayesian optimization: utilizado para optimizar hiperparámetros\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import early_stopping  # Early stopping: utilizado para evitar sobreajuste\n",
    "\n",
    "from Funcoes_Comuns import avaliar_modelo, registrar_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400e96f",
   "metadata": {},
   "source": [
    "### Leitura de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52895d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\Finais\\enem_microdados_2023.pkl')\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categóricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para validação cruzada (15% do conjunto de treino)\n",
    "X_train_final, X_eval, y_train_final, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22258",
   "metadata": {},
   "source": [
    "### Buscar melhorar modelo alterando Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a26e9d",
   "metadata": {},
   "source": [
    "Tentativas de melhorar o resultado alterando as features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac00f3b",
   "metadata": {},
   "source": [
    "#### a. Tentar melhorar removendo todas as variáveis sem categorias definidas (\"Não informado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b537a1",
   "metadata": {},
   "source": [
    "    TP_ESTADO_CIVIL: 0 (Não informado)\n",
    "    TP_COR_RACA: 0 (Não declarado)\n",
    "    TP_NACIONALIDADE: 0 (Não informado)\n",
    "    TP_ESCOLA: 1 (Não Respondeu)\n",
    "    TP_ENSINO: 0 (Não informado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47008f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover linhas com valores não explicativos\n",
    "mascaras_validas_train = (\n",
    "    (X_train['CAT_NACIONALIDADE'] != 0) &\n",
    "    (X_train['CAT_ENSINO'] != 0) &\n",
    "    (X_train['CAT_COR_RACA'] != 0) &\n",
    "    (X_train['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (X_train['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "X_train_a = X_train[mascaras_validas_train]\n",
    "y_train_a = y_train.loc[X_train_a.index]\n",
    "\n",
    "mascaras_validas_train_final = (\n",
    "    (X_train_final['CAT_NACIONALIDADE'] != 0) &\n",
    "    (X_train_final['CAT_ENSINO'] != 0) &\n",
    "    (X_train_final['CAT_COR_RACA'] != 0) &\n",
    "    (X_train_final['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (X_train_final['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "X_train_final_a = X_train_final[mascaras_validas_train_final]\n",
    "y_train_final_a = y_train_final.loc[X_train_final_a.index]\n",
    "\n",
    "mascaras_validas_eval = (\n",
    "    (X_eval['CAT_NACIONALIDADE'] != 0) &\n",
    "    (X_eval['CAT_ENSINO'] != 0) &\n",
    "    (X_eval['CAT_COR_RACA'] != 0) &\n",
    "    (X_eval['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (X_eval['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "X_eval_a = X_eval[mascaras_validas_eval]\n",
    "y_eval_a = y_eval.loc[X_eval_a.index]\n",
    "\n",
    "X_test_a = X_test.copy()\n",
    "y_test_a = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9f9dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Info] Total Bins 4245\n",
      "[LightGBM] [Info] Number of data points in the train set: 537944, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 529.366449\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_a = lgbm.LGBMRegressor(random_state=42,\n",
    "                                max_bin=4095, \n",
    "                                force_row_wise=True)\n",
    "\n",
    "# Definição do espaço de busca para otimização bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (52, 60),                         # Número de folhas na árvore de decisão\n",
    "    'max_depth': (60, 100),                         # Profundidade máxima da árvore\n",
    "    'learning_rate': (0.001, 0.01, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                   # Número de árvores\n",
    "    'subsample': (0.5, 0.9),                        # Proporção de amostras usadas em cada árvore\n",
    "    'colsample_bytree': (0.3, 0.9),                 # Fração de colunas a serem usadas por árvore\n",
    "    'reg_alpha': (1e-3, 1e-1, 'log-uniform'),       # Regularização L1\n",
    "    'reg_lambda': (1e-6, 1e-4, 'log-uniform'),      # Regularização L2\n",
    "}\n",
    "\n",
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_a,             # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espaço de busca definido acima\n",
    "    scoring='r2',                   # Critério de seleção\n",
    "    n_iter=5,                       # Número de avaliações do modelo\n",
    "    cv=3,                           # Validação cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paralelização total dos cálculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # Métricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categóricas\n",
    "}\n",
    "\n",
    "# Executar a busca Bayesiana\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_a, y_train_a['NUM_NOTA_CH'], **fit_params)\n",
    "# Parar o cronômetro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd97f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: OrderedDict([('colsample_bytree', 0.5460623753119883), ('learning_rate', 0.0053422688874711095), ('max_depth', 97), ('n_estimators', 5316), ('num_leaves', 57), ('reg_alpha', 0.006733444192152367), ('reg_lambda', 5.033414197773552e-06), ('subsample', 0.7958016936761683)])\n",
      "R2:  0.3113621349059856\n",
      "Tempo total de execução: 12681.03 segundos\n"
     ]
    }
   ],
   "source": [
    "# Melhores parâmetros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores parâmetros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execução: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\": 0.5460623753119883,\n",
    "        \"learning_rate\": 0.0053422688874711095,\n",
    "        \"max_depth\": 97,\n",
    "        \"n_estimators\": 5316,\n",
    "        \"num_leaves\": 57,\n",
    "        \"reg_alpha\": 0.006733444192152367,\n",
    "        \"reg_lambda\": 5.033414197773552e-06,\n",
    "        \"subsample\": 0.7958016936761683\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores parâmetros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30471290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4285\n",
      "[LightGBM] [Info] Number of data points in the train set: 457240, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 529.301688\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2257]\tvalid_0's rmse: 69.9511\tvalid_0's l1: 55.1395\tvalid_0's l2: 4893.16\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores parâmetros encontrados\n",
    "modelo_a.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores parâmetros encontrados\n",
    "modelo_a.fit(X_train_final_a, \n",
    "                y_train_final_a['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval_a, y_eval_a['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features,\n",
    "                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c532044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "y_pred_explicativas = modelo_a.predict(X_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f8a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas' already exists. Creating a new version of this model...\n",
      "2025/06/14 17:56:59 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas, version 5\n",
      "Created version '5' of model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run unique-colt-427 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/944f331f4a2c4cd9a13ae44641f9504a\n",
      "🧪 View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_a,\n",
    "                    parametros={**modelo_a.get_params(), \"amostra\": X_train_a.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train_a,\n",
    "                    y_train=y_train_a,\n",
    "                    y_test=y_test_a,\n",
    "                    y_pred=y_pred_explicativas,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV sem categorias definidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12cfc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.1030\n",
      "RMSE (treino): 67.4584\n",
      "R2 (treino): 0.3613\n",
      "MAE (teste): 55.0613\n",
      "RMSE (teste): 69.8204\n",
      "R2 (teste): 0.3162\n"
     ]
    }
   ],
   "source": [
    "# Avaliação grupo treino\n",
    "avaliar_modelo(y_train_a['NUM_NOTA_CH'], modelo_a.predict(X_train_a), \"treino\")\n",
    "\n",
    "# Avaliação grupo teste\n",
    "avaliar_modelo(y_test_a['NUM_NOTA_CH'], y_pred_explicativas, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912a77f",
   "metadata": {},
   "source": [
    "### 2. Tentar melhorar removendo variáveis de menor importância (importância e informação mútua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040510fd",
   "metadata": {},
   "source": [
    "    CAT_SIT_FUNC_ESC\n",
    "    CAT_NACIONALIDADE\n",
    "    CAT_ENSINO\n",
    "    CAT_LOCALIZACAO_ESC\n",
    "    CAT_ESTADO_CIVIL\n",
    "    BIN_Q002_DUMMY_H\n",
    "    NUM_Q017\n",
    "    NUM_Q012\n",
    "    BIN_Q001_DUMMY_H\n",
    "    NUM_Q015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2cd468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover algumas colunas com MENOR valor explicativo\n",
    "menos_importantes=['CAT_SIT_FUNC_ESC',\n",
    "                    'CAT_NACIONALIDADE',\n",
    "                    'CAT_ENSINO',\n",
    "                    'CAT_LOCALIZACAO_ESC',\n",
    "                    'CAT_ESTADO_CIVIL',\n",
    "                    'BIN_Q002_DUMMY_H',\n",
    "                    'NUM_Q017',\n",
    "                    'NUM_Q012',\n",
    "                    'BIN_Q001_DUMMY_H',\n",
    "                    'NUM_Q015',\n",
    "                ]\n",
    "\n",
    "X_train_b = X_train.drop(columns=menos_importantes).copy()\n",
    "y_train_b = y_train.copy()\n",
    "\n",
    "X_train_final_b = X_train_final.drop(columns=menos_importantes).copy()\n",
    "y_train_final_b = y_train_final.copy()\n",
    "\n",
    "X_eval_b = X_eval.drop(columns=menos_importantes).copy()\n",
    "y_eval_b = y_eval.copy()\n",
    "\n",
    "X_test_b = X_test.drop(columns=menos_importantes).copy()\n",
    "y_test_b = y_test.copy()\n",
    "\n",
    "# Obter colunas categóricas\n",
    "categorical_features = X_train_b.select_dtypes(include=['category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec967891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Info] Total Bins 4227\n",
      "[LightGBM] [Info] Number of data points in the train set: 573194, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 527.941622\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_b = lgbm.LGBMRegressor(random_state=42,\n",
    "                                max_bin=4095, \n",
    "                                force_row_wise=True)\n",
    "\n",
    "# Definição do espaço de busca para otimização bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (52, 60),                         # Número de folhas na árvore de decisão\n",
    "    'max_depth': (60, 100),                         # Profundidade máxima da árvore\n",
    "    'learning_rate': (0.001, 0.01, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                   # Número de árvores\n",
    "    'subsample': (0.5, 0.9),                        # Proporção de amostras usadas em cada árvore\n",
    "    'colsample_bytree': (0.3, 0.9),                 # Fração de colunas a serem usadas por árvore\n",
    "    'reg_alpha': (1e-3, 1e-1, 'log-uniform'),       # Regularização L1\n",
    "    'reg_lambda': (1e-6, 1e-4, 'log-uniform'),      # Regularização L2\n",
    "}\n",
    "\n",
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_b,             # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espaço de busca definido acima\n",
    "    scoring='r2',                   # Critério de seleção\n",
    "    n_iter=5,                       # Número de avaliações do modelo\n",
    "    cv=3,                           # Validação cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paralelização total dos cálculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # Métricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categóricas\n",
    "}\n",
    "\n",
    "# Executar a busca Bayesiana\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_b, y_train_b['NUM_NOTA_CH'], **fit_params)\n",
    "# Parar o cronômetro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b5f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: OrderedDict([('colsample_bytree', 0.5460623753119883), ('learning_rate', 0.0053422688874711095), ('max_depth', 97), ('n_estimators', 5316), ('num_leaves', 57), ('reg_alpha', 0.006733444192152367), ('reg_lambda', 5.033414197773552e-06), ('subsample', 0.7958016936761683)])\n",
      "R2:  0.3112070763981326\n",
      "Tempo total de execução: 48518.06 segundos\n"
     ]
    }
   ],
   "source": [
    "# Melhores parâmetros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores parâmetros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execução: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\":  0.5460623753119883,\n",
    "        \"learning_rate\": 0.0053422688874711095,\n",
    "        \"max_depth\": 97,\n",
    "        \"n_estimators\": 5316,\n",
    "        \"num_leaves\": 57,\n",
    "        \"reg_alpha\": 0.006733444192152367,\n",
    "        \"reg_lambda\": 5.033414197773552e-06,\n",
    "        \"subsample\": 0.7958016936761683\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores parâmetros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9d39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4258\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 527.881246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1995]\tvalid_0's rmse: 70.192\tvalid_0's l1: 55.3924\tvalid_0's l2: 4926.92\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores parâmetros encontrados\n",
    "modelo_b.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores parâmetros encontrados\n",
    "modelo_b.fit(X_train_final_b,\n",
    "                y_train_final_b['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval_b, y_eval_b['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features,\n",
    "                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ea254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "y_pred_var_importantes = modelo_b.predict(X_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8485081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/06/15 08:32:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ferra\\AppData\\Local\\Temp\\tmppix6djib\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.6.1', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "Registered model 'modelo_lgbm_reducao_de_10_features' already exists. Creating a new version of this model...\n",
      "2025/06/15 08:33:00 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_reducao_de_10_features, version 4\n",
      "Created version '4' of model 'modelo_lgbm_reducao_de_10_features'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run puzzled-gnat-547 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/a0364b405b1640b3b697c0a0d12ec036\n",
      "🧪 View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_reducao_de_10_features\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_b,\n",
    "                    parametros={**modelo_b.get_params(), \"amostra\": X_train_b.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train_b,\n",
    "                    y_train=y_train_b,\n",
    "                    y_test=y_test_b,\n",
    "                    y_pred=y_pred_var_importantes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redução de 10 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2833d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.5820\n",
      "RMSE (treino): 67.9726\n",
      "R2 (treino): 0.3553\n",
      "MAE (teste): 55.1367\n",
      "RMSE (teste): 69.9228\n",
      "R2 (teste): 0.3142\n"
     ]
    }
   ],
   "source": [
    "# Avaliação grupo treino\n",
    "avaliar_modelo(y_train_b['NUM_NOTA_CH'], modelo_b.predict(X_train_b), \"treino\")\n",
    "# Avaliação grupo teste\n",
    "avaliar_modelo(y_test_b['NUM_NOTA_CH'], y_pred_var_importantes, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371aee35",
   "metadata": {},
   "source": [
    "### 3. Aplicar os dois: remover categorias não explicativas + remover colunas menos importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fc6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Máscara para linhas válidas\n",
    "mascara_validas = lambda df: (\n",
    "    (df['CAT_NACIONALIDADE'] != 0) &\n",
    "    (df['CAT_ENSINO'] != 0) &\n",
    "    (df['CAT_COR_RACA'] != 0) &\n",
    "    (df['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (df['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "\n",
    "menos_importantes = [\n",
    "    'CAT_SIT_FUNC_ESC', 'CAT_NACIONALIDADE', 'CAT_ENSINO', 'CAT_LOCALIZACAO_ESC',\n",
    "    'CAT_ESTADO_CIVIL', 'BIN_Q002_DUMMY_H', 'NUM_Q017', 'NUM_Q012',\n",
    "    'BIN_Q001_DUMMY_H', 'NUM_Q015'\n",
    "]\n",
    "\n",
    "# Aplicar máscara e remover colunas em uma linha para cada conjunto\n",
    "X_train_c = X_train[mascara_validas(X_train)].drop(columns=menos_importantes)\n",
    "y_train_c = y_train.loc[X_train_c.index]\n",
    "\n",
    "X_train_final_c = X_train_final[mascara_validas(X_train_final)].drop(columns=menos_importantes)\n",
    "y_train_final_c = y_train_final.loc[X_train_final_c.index]\n",
    "\n",
    "X_eval_c = X_eval[mascara_validas(X_eval)].drop(columns=menos_importantes)\n",
    "y_eval_c = y_eval.loc[X_eval_c.index]\n",
    "\n",
    "X_test_c = X_test.drop(columns=menos_importantes)\n",
    "y_test_c = y_test.copy()\n",
    "\n",
    "# Obter colunas categóricas\n",
    "categorical_features = X_train_c.select_dtypes(include=['category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51460c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Info] Total Bins 4209\n",
      "[LightGBM] [Info] Number of data points in the train set: 537944, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 529.366449\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_c = lgbm.LGBMRegressor(random_state=42,\n",
    "                                max_bin=4095, \n",
    "                                force_row_wise=True)\n",
    "\n",
    "# Definição do espaço de busca para otimização bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (52, 60),                         # Número de folhas na árvore de decisão\n",
    "    'max_depth': (60, 100),                         # Profundidade máxima da árvore\n",
    "    'learning_rate': (0.001, 0.01, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                   # Número de árvores\n",
    "    'subsample': (0.5, 0.9),                        # Proporção de amostras usadas em cada árvore\n",
    "    'colsample_bytree': (0.3, 0.9),                 # Fração de colunas a serem usadas por árvore\n",
    "    'reg_alpha': (1e-3, 1e-1, 'log-uniform'),       # Regularização L1\n",
    "    'reg_lambda': (1e-6, 1e-4, 'log-uniform'),      # Regularização L2\n",
    "}\n",
    "\n",
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_c,             # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espaço de busca definido acima\n",
    "    scoring='r2',                   # Critério de seleção\n",
    "    n_iter=5,                       # Número de avaliações do modelo\n",
    "    cv=3,                           # Validação cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paralelização total dos cálculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # Métricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categóricas\n",
    "}\n",
    "\n",
    "# Executar a busca Bayesiana\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_c, y_train_c['NUM_NOTA_CH'], **fit_params)\n",
    "# Parar o cronômetro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a697da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: OrderedDict([('colsample_bytree', 0.5460623753119883), ('learning_rate', 0.0053422688874711095), ('max_depth', 97), ('n_estimators', 5316), ('num_leaves', 57), ('reg_alpha', 0.006733444192152367), ('reg_lambda', 5.033414197773552e-06), ('subsample', 0.7958016936761683)])\n",
      "R2:  0.3086380129697774\n",
      "Tempo total de execução: 30557.31 segundos\n"
     ]
    }
   ],
   "source": [
    "# Melhores parâmetros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores parâmetros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execução: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\": 0.5460623753119883,\n",
    "        \"learning_rate\": 0.0053422688874711095,\n",
    "        \"max_depth\": 97,\n",
    "        \"n_estimators\": 5316,\n",
    "        \"num_leaves\": 57,\n",
    "        \"reg_alpha\": 0.006733444192152367,\n",
    "        \"reg_lambda\": 5.033414197773552e-06,\n",
    "        \"subsample\": 0.7958016936761683\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores parâmetros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b3490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4249\n",
      "[LightGBM] [Info] Number of data points in the train set: 457240, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 529.301688\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2006]\tvalid_0's rmse: 70.0653\tvalid_0's l1: 55.2435\tvalid_0's l2: 4909.15\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores parâmetros encontrados\n",
    "modelo_c.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores parâmetros encontrados\n",
    "modelo_c.fit(X_train_final_c,\n",
    "                y_train_final_c['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval_c, y_eval_c['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features,\n",
    "                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5918d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "y_pred_var_feat_categ = modelo_c.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5c102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas' already exists. Creating a new version of this model...\n",
      "2025/06/15 17:19:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas, version 2\n",
      "Created version '2' of model 'modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run judicious-boar-636 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/28dd05e984c14bc89eac86b6d35233d6\n",
      "🧪 View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_c,\n",
    "                    parametros={**modelo_c.get_params(), \"amostra\": X_train_c.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train_c,\n",
    "                    y_train=y_train_c,\n",
    "                    y_test=y_test_c,\n",
    "                    y_pred=y_pred_var_feat_categ,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redução de 10 features e categorias não explicativas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4b89abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.3819\n",
      "RMSE (treino): 67.7830\n",
      "R2 (treino): 0.3551\n",
      "MAE (teste): 55.1422\n",
      "RMSE (teste): 69.9457\n",
      "R2 (teste): 0.3137\n"
     ]
    }
   ],
   "source": [
    "# Avaliação grupo treino\n",
    "avaliar_modelo(y_train_c['NUM_NOTA_CH'], modelo_c.predict(X_train_c), \"treino\")\n",
    "# Avaliação grupo teste\n",
    "avaliar_modelo(y_test_c['NUM_NOTA_CH'], y_pred_var_feat_categ, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf74892",
   "metadata": {},
   "source": [
    "Conclusão\n",
    "\n",
    "- Remoção de categorias não explicativas\n",
    "    - removendo apenas as linhas com categorias não explicativas houve uma piora do modelo\n",
    "    - R² de 0,3201 para 0.3162\n",
    "\n",
    "- Remoção das 10 variáveis menos explicativas\n",
    "    - removendo apenas as colunas com menor importância para o alvo houve uma piora do modelo\n",
    "    - Removendo 10: R² de 0,3201 para 0,3142\n",
    "\n",
    "- Aplicando as duas ténicas\n",
    "    - Redução R² de 0,3201 para 0,3137\n",
    "\n",
    "Padrão se mantém para outras métricas RMSE e MAE\n",
    "\n",
    "Ambas abordagens não melhoram o modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
