{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d08606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV  # Bayesian optimization: utilizado para optimizar hiperpar√°metros\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import early_stopping  # Early stopping: utilizado para evitar sobreajuste\n",
    "\n",
    "from Funcoes_Comuns import avaliar_modelo, registrar_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400e96f",
   "metadata": {},
   "source": [
    "### Leitura de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52895d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\Finais\\enem_microdados_2023.pkl')\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_final, X_eval, y_train_final, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22258",
   "metadata": {},
   "source": [
    "### Buscar melhorar modelo alterando Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a26e9d",
   "metadata": {},
   "source": [
    "Tentativas de melhorar o resultado alterando as features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac00f3b",
   "metadata": {},
   "source": [
    "#### a. Tentar melhorar removendo todas as vari√°veis sem categorias definidas (\"N√£o informado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b537a1",
   "metadata": {},
   "source": [
    "    TP_ESTADO_CIVIL: 0 (N√£o informado)\n",
    "    TP_COR_RACA: 0 (N√£o declarado)\n",
    "    TP_NACIONALIDADE: 0 (N√£o informado)\n",
    "    TP_ESCOLA: 1 (N√£o Respondeu)\n",
    "    TP_ENSINO: 0 (N√£o informado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47008f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover linhas com valores n√£o explicativos\n",
    "mascaras_validas_train = (\n",
    "    (X_train['CAT_NACIONALIDADE'] != 0) &\n",
    "    (X_train['CAT_ENSINO'] != 0) &\n",
    "    (X_train['CAT_COR_RACA'] != 0) &\n",
    "    (X_train['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (X_train['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "X_train_a = X_train[mascaras_validas_train]\n",
    "y_train_a = y_train.loc[X_train_a.index]\n",
    "\n",
    "mascaras_validas_train_final = (\n",
    "    (X_train_final['CAT_NACIONALIDADE'] != 0) &\n",
    "    (X_train_final['CAT_ENSINO'] != 0) &\n",
    "    (X_train_final['CAT_COR_RACA'] != 0) &\n",
    "    (X_train_final['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (X_train_final['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "X_train_final_a = X_train_final[mascaras_validas_train_final]\n",
    "y_train_final_a = y_train_final.loc[X_train_final_a.index]\n",
    "\n",
    "mascaras_validas_eval = (\n",
    "    (X_eval['CAT_NACIONALIDADE'] != 0) &\n",
    "    (X_eval['CAT_ENSINO'] != 0) &\n",
    "    (X_eval['CAT_COR_RACA'] != 0) &\n",
    "    (X_eval['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (X_eval['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "X_eval_a = X_eval[mascaras_validas_eval]\n",
    "y_eval_a = y_eval.loc[X_eval_a.index]\n",
    "\n",
    "X_test_a = X_test.copy()\n",
    "y_test_a = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9f9dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Info] Total Bins 4245\n",
      "[LightGBM] [Info] Number of data points in the train set: 537944, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 529.366449\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_a = lgbm.LGBMRegressor(random_state=42,\n",
    "                                max_bin=4095, \n",
    "                                force_row_wise=True)\n",
    "\n",
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (52, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (60, 100),                         # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.001, 0.01, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                   # N√∫mero de √°rvores\n",
    "    'subsample': (0.5, 0.9),                        # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.3, 0.9),                 # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1e-1, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-6, 1e-4, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}\n",
    "\n",
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_a,             # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=5,                       # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=3,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categ√≥ricas\n",
    "}\n",
    "\n",
    "# Executar a busca Bayesiana\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_a, y_train_a['NUM_NOTA_CH'], **fit_params)\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd97f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores par√¢metros: OrderedDict([('colsample_bytree', 0.5460623753119883), ('learning_rate', 0.0053422688874711095), ('max_depth', 97), ('n_estimators', 5316), ('num_leaves', 57), ('reg_alpha', 0.006733444192152367), ('reg_lambda', 5.033414197773552e-06), ('subsample', 0.7958016936761683)])\n",
      "R2:  0.3113621349059856\n",
      "Tempo total de execu√ß√£o: 12681.03 segundos\n"
     ]
    }
   ],
   "source": [
    "# Melhores par√¢metros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores par√¢metros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\": 0.5460623753119883,\n",
    "        \"learning_rate\": 0.0053422688874711095,\n",
    "        \"max_depth\": 97,\n",
    "        \"n_estimators\": 5316,\n",
    "        \"num_leaves\": 57,\n",
    "        \"reg_alpha\": 0.006733444192152367,\n",
    "        \"reg_lambda\": 5.033414197773552e-06,\n",
    "        \"subsample\": 0.7958016936761683\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores par√¢metros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30471290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4285\n",
      "[LightGBM] [Info] Number of data points in the train set: 457240, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 529.301688\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2257]\tvalid_0's rmse: 69.9511\tvalid_0's l1: 55.1395\tvalid_0's l2: 4893.16\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_a.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_a.fit(X_train_final_a, \n",
    "                y_train_final_a['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval_a, y_eval_a['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features,\n",
    "                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c532044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_explicativas = modelo_a.predict(X_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f8a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas' already exists. Creating a new version of this model...\n",
      "2025/06/14 17:56:59 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas, version 5\n",
      "Created version '5' of model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run unique-colt-427 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/944f331f4a2c4cd9a13ae44641f9504a\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_a,\n",
    "                    parametros={**modelo_a.get_params(), \"amostra\": X_train_a.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train_a,\n",
    "                    y_train=y_train_a,\n",
    "                    y_test=y_test_a,\n",
    "                    y_pred=y_pred_explicativas,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV sem categorias definidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12cfc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.1030\n",
      "RMSE (treino): 67.4584\n",
      "R2 (treino): 0.3613\n",
      "MAE (teste): 55.0613\n",
      "RMSE (teste): 69.8204\n",
      "R2 (teste): 0.3162\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train_a['NUM_NOTA_CH'], modelo_a.predict(X_train_a), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test_a['NUM_NOTA_CH'], y_pred_explicativas, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912a77f",
   "metadata": {},
   "source": [
    "### 2. Tentar melhorar removendo vari√°veis de menor import√¢ncia (import√¢ncia e informa√ß√£o m√∫tua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040510fd",
   "metadata": {},
   "source": [
    "    CAT_SIT_FUNC_ESC\n",
    "    CAT_NACIONALIDADE\n",
    "    CAT_ENSINO\n",
    "    CAT_LOCALIZACAO_ESC\n",
    "    CAT_ESTADO_CIVIL\n",
    "    BIN_Q002_DUMMY_H\n",
    "    NUM_Q017\n",
    "    NUM_Q012\n",
    "    BIN_Q001_DUMMY_H\n",
    "    NUM_Q015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2cd468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover algumas colunas com MENOR valor explicativo\n",
    "menos_importantes=['CAT_SIT_FUNC_ESC',\n",
    "                    'CAT_NACIONALIDADE',\n",
    "                    'CAT_ENSINO',\n",
    "                    'CAT_LOCALIZACAO_ESC',\n",
    "                    'CAT_ESTADO_CIVIL',\n",
    "                    'BIN_Q002_DUMMY_H',\n",
    "                    'NUM_Q017',\n",
    "                    'NUM_Q012',\n",
    "                    'BIN_Q001_DUMMY_H',\n",
    "                    'NUM_Q015',\n",
    "                ]\n",
    "\n",
    "X_train_b = X_train.drop(columns=menos_importantes).copy()\n",
    "y_train_b = y_train.copy()\n",
    "\n",
    "X_train_final_b = X_train_final.drop(columns=menos_importantes).copy()\n",
    "y_train_final_b = y_train_final.copy()\n",
    "\n",
    "X_eval_b = X_eval.drop(columns=menos_importantes).copy()\n",
    "y_eval_b = y_eval.copy()\n",
    "\n",
    "X_test_b = X_test.drop(columns=menos_importantes).copy()\n",
    "y_test_b = y_test.copy()\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train_b.select_dtypes(include=['category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec967891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Info] Total Bins 4227\n",
      "[LightGBM] [Info] Number of data points in the train set: 573194, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 527.941622\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_b = lgbm.LGBMRegressor(random_state=42,\n",
    "                                max_bin=4095, \n",
    "                                force_row_wise=True)\n",
    "\n",
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (52, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (60, 100),                         # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.001, 0.01, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                   # N√∫mero de √°rvores\n",
    "    'subsample': (0.5, 0.9),                        # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.3, 0.9),                 # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1e-1, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-6, 1e-4, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}\n",
    "\n",
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_b,             # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=5,                       # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=3,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categ√≥ricas\n",
    "}\n",
    "\n",
    "# Executar a busca Bayesiana\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_b, y_train_b['NUM_NOTA_CH'], **fit_params)\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b5f894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores par√¢metros: OrderedDict([('colsample_bytree', 0.5460623753119883), ('learning_rate', 0.0053422688874711095), ('max_depth', 97), ('n_estimators', 5316), ('num_leaves', 57), ('reg_alpha', 0.006733444192152367), ('reg_lambda', 5.033414197773552e-06), ('subsample', 0.7958016936761683)])\n",
      "R2:  0.3112070763981326\n",
      "Tempo total de execu√ß√£o: 48518.06 segundos\n"
     ]
    }
   ],
   "source": [
    "# Melhores par√¢metros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores par√¢metros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\":  0.5460623753119883,\n",
    "        \"learning_rate\": 0.0053422688874711095,\n",
    "        \"max_depth\": 97,\n",
    "        \"n_estimators\": 5316,\n",
    "        \"num_leaves\": 57,\n",
    "        \"reg_alpha\": 0.006733444192152367,\n",
    "        \"reg_lambda\": 5.033414197773552e-06,\n",
    "        \"subsample\": 0.7958016936761683\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores par√¢metros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9d39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4258\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 527.881246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1995]\tvalid_0's rmse: 70.192\tvalid_0's l1: 55.3924\tvalid_0's l2: 4926.92\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_b.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_b.fit(X_train_final_b,\n",
    "                y_train_final_b['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval_b, y_eval_b['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features,\n",
    "                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ea254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_var_importantes = modelo_b.predict(X_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8485081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/06/15 08:32:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ferra\\AppData\\Local\\Temp\\tmppix6djib\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.6.1', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "Registered model 'modelo_lgbm_reducao_de_10_features' already exists. Creating a new version of this model...\n",
      "2025/06/15 08:33:00 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_reducao_de_10_features, version 4\n",
      "Created version '4' of model 'modelo_lgbm_reducao_de_10_features'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run puzzled-gnat-547 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/a0364b405b1640b3b697c0a0d12ec036\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_reducao_de_10_features\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_b,\n",
    "                    parametros={**modelo_b.get_params(), \"amostra\": X_train_b.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train_b,\n",
    "                    y_train=y_train_b,\n",
    "                    y_test=y_test_b,\n",
    "                    y_pred=y_pred_var_importantes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redu√ß√£o de 10 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2833d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.5820\n",
      "RMSE (treino): 67.9726\n",
      "R2 (treino): 0.3553\n",
      "MAE (teste): 55.1367\n",
      "RMSE (teste): 69.9228\n",
      "R2 (teste): 0.3142\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train_b['NUM_NOTA_CH'], modelo_b.predict(X_train_b), \"treino\")\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test_b['NUM_NOTA_CH'], y_pred_var_importantes, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371aee35",
   "metadata": {},
   "source": [
    "### 3. Aplicar os dois: remover categorias n√£o explicativas + remover colunas menos importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fc6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√°scara para linhas v√°lidas\n",
    "mascara_validas = lambda df: (\n",
    "    (df['CAT_NACIONALIDADE'] != 0) &\n",
    "    (df['CAT_ENSINO'] != 0) &\n",
    "    (df['CAT_COR_RACA'] != 0) &\n",
    "    (df['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (df['CAT_ESCOLA'] != 1)\n",
    ")\n",
    "\n",
    "menos_importantes = [\n",
    "    'CAT_SIT_FUNC_ESC', 'CAT_NACIONALIDADE', 'CAT_ENSINO', 'CAT_LOCALIZACAO_ESC',\n",
    "    'CAT_ESTADO_CIVIL', 'BIN_Q002_DUMMY_H', 'NUM_Q017', 'NUM_Q012',\n",
    "    'BIN_Q001_DUMMY_H', 'NUM_Q015'\n",
    "]\n",
    "\n",
    "# Aplicar m√°scara e remover colunas em uma linha para cada conjunto\n",
    "X_train_c = X_train[mascara_validas(X_train)].drop(columns=menos_importantes)\n",
    "y_train_c = y_train.loc[X_train_c.index]\n",
    "\n",
    "X_train_final_c = X_train_final[mascara_validas(X_train_final)].drop(columns=menos_importantes)\n",
    "y_train_final_c = y_train_final.loc[X_train_final_c.index]\n",
    "\n",
    "X_eval_c = X_eval[mascara_validas(X_eval)].drop(columns=menos_importantes)\n",
    "y_eval_c = y_eval.loc[X_eval_c.index]\n",
    "\n",
    "X_test_c = X_test.drop(columns=menos_importantes)\n",
    "y_test_c = y_test.copy()\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train_c.select_dtypes(include=['category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51460c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[LightGBM] [Info] Total Bins 4209\n",
      "[LightGBM] [Info] Number of data points in the train set: 537944, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 529.366449\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_c = lgbm.LGBMRegressor(random_state=42,\n",
    "                                max_bin=4095, \n",
    "                                force_row_wise=True)\n",
    "\n",
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (52, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (60, 100),                         # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.001, 0.01, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                   # N√∫mero de √°rvores\n",
    "    'subsample': (0.5, 0.9),                        # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.3, 0.9),                 # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1e-1, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-6, 1e-4, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}\n",
    "\n",
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_c,             # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=5,                       # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=3,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categ√≥ricas\n",
    "}\n",
    "\n",
    "# Executar a busca Bayesiana\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_c, y_train_c['NUM_NOTA_CH'], **fit_params)\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a697da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores par√¢metros: OrderedDict([('colsample_bytree', 0.5460623753119883), ('learning_rate', 0.0053422688874711095), ('max_depth', 97), ('n_estimators', 5316), ('num_leaves', 57), ('reg_alpha', 0.006733444192152367), ('reg_lambda', 5.033414197773552e-06), ('subsample', 0.7958016936761683)])\n",
      "R2:  0.3086380129697774\n",
      "Tempo total de execu√ß√£o: 30557.31 segundos\n"
     ]
    }
   ],
   "source": [
    "# Melhores par√¢metros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores par√¢metros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\": 0.5460623753119883,\n",
    "        \"learning_rate\": 0.0053422688874711095,\n",
    "        \"max_depth\": 97,\n",
    "        \"n_estimators\": 5316,\n",
    "        \"num_leaves\": 57,\n",
    "        \"reg_alpha\": 0.006733444192152367,\n",
    "        \"reg_lambda\": 5.033414197773552e-06,\n",
    "        \"subsample\": 0.7958016936761683\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores par√¢metros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b3490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4249\n",
      "[LightGBM] [Info] Number of data points in the train set: 457240, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 529.301688\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2006]\tvalid_0's rmse: 70.0653\tvalid_0's l1: 55.2435\tvalid_0's l2: 4909.15\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_c.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_c.fit(X_train_final_c,\n",
    "                y_train_final_c['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval_c, y_eval_c['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features,\n",
    "                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5918d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_var_feat_categ = modelo_c.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5c102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas' already exists. Creating a new version of this model...\n",
      "2025/06/15 17:19:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas, version 2\n",
      "Created version '2' of model 'modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run judicious-boar-636 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/28dd05e984c14bc89eac86b6d35233d6\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_c,\n",
    "                    parametros={**modelo_c.get_params(), \"amostra\": X_train_c.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train_c,\n",
    "                    y_train=y_train_c,\n",
    "                    y_test=y_test_c,\n",
    "                    y_pred=y_pred_var_feat_categ,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redu√ß√£o de 10 features e categorias n√£o explicativas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4b89abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.3819\n",
      "RMSE (treino): 67.7830\n",
      "R2 (treino): 0.3551\n",
      "MAE (teste): 55.1422\n",
      "RMSE (teste): 69.9457\n",
      "R2 (teste): 0.3137\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train_c['NUM_NOTA_CH'], modelo_c.predict(X_train_c), \"treino\")\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test_c['NUM_NOTA_CH'], y_pred_var_feat_categ, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf74892",
   "metadata": {},
   "source": [
    "Conclus√£o\n",
    "\n",
    "- Remo√ß√£o de categorias n√£o explicativas\n",
    "    - removendo apenas as linhas com categorias n√£o explicativas houve uma piora do modelo\n",
    "    - R¬≤ de 0,3201 para 0.3162\n",
    "\n",
    "- Remo√ß√£o das 10 vari√°veis menos explicativas\n",
    "    - removendo apenas as colunas com menor import√¢ncia para o alvo houve uma piora do modelo\n",
    "    - Removendo 10: R¬≤ de 0,3201 para 0,3142\n",
    "\n",
    "- Aplicando as duas t√©nicas\n",
    "    - Redu√ß√£o R¬≤ de 0,3201 para 0,3137\n",
    "\n",
    "Padr√£o se mant√©m para outras m√©tricas RMSE e MAE\n",
    "\n",
    "Ambas abordagens n√£o melhoram o modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
