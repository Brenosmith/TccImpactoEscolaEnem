{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08652991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV  # Bayesian optimization: utilizado para optimizar hiperpar√°metros\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import early_stopping  # Early stopping: utilizado para evitar sobreajuste\n",
    "\n",
    "from Funcoes_Comuns import avaliar_modelo, registrar_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10e0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\microdados_enem_censo_2023.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81789cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e4bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste de tipo para MLflow\n",
    "# Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43f32ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAT_COR_RACA',\n",
       " 'CAT_CO_UF_ESC',\n",
       " 'CAT_ENSINO',\n",
       " 'CAT_ESCOLA',\n",
       " 'CAT_ESTADO_CIVIL',\n",
       " 'CAT_FAIXA_ETARIA',\n",
       " 'CAT_LINGUA',\n",
       " 'CAT_NACIONALIDADE',\n",
       " 'CAT_Q003',\n",
       " 'CAT_Q004',\n",
       " 'CAT_SEXO',\n",
       " 'CAT_SIT_FUNC_ESC',\n",
       " 'CAT_MODE_CATEGORIA_ESCOLA_PRIVADA',\n",
       " 'CAT_MODE_EXAME_SELECAO',\n",
       " 'CAT_MODE_LOCALIZACAO_DIFERENCIADA',\n",
       " 'CAT_MODE_OCUPACAO_GALPAO',\n",
       " 'CAT_MODE_OCUPACAO_PREDIO_ESCOLAR',\n",
       " 'CAT_MODE_ORGAO_REGIONAL',\n",
       " 'CAT_MODE_PROPOSTA_PEDAGOGICA',\n",
       " 'CAT_MODE_REGIAO',\n",
       " 'CAT_MODE_TRATAMENTO_LIXO_INEXISTENTE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682f9e5",
   "metadata": {},
   "source": [
    "Modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edffdbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 29930\n",
      "[LightGBM] [Info] Number of data points in the train set: 573194, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score 527.941622\n"
     ]
    }
   ],
   "source": [
    "# Treinar modelo LGBMRegressor Base\n",
    "modelo_lgbm = lgbm.LGBMRegressor(n_estimators=1000, \n",
    "                                 learning_rate=0.01, \n",
    "                                 random_state=42,\n",
    "                                 max_bin=4095,\n",
    "                                 force_row_wise=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "modelo_lgbm.fit(X_train, \n",
    "                y_train['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_test, y_test['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features)\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855be84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred = modelo_lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05a3157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/04 21:05:34 WARNING mlflow.models.signature: Failed to infer schema for inputs. Setting schema to `Schema([ColSpec(type=AnyType())]` as default. Note that MLflow doesn't validate data types during inference for AnyType. To see the full traceback, set logging level to DEBUG.\n",
      "Successfully registered model 'modelo_lgbm_base_censo_enem'.\n",
      "2025/06/04 21:06:03 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_base_censo_enem, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run brawny-sloth-372 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/f013ab98f0f848cf89f15b5ac088cd36\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_base_censo_enem\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'modelo_lgbm_base_censo_enem'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                 parametros={**modelo_lgbm.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                 X_train=X_train,\n",
    "                 y_train=y_train,\n",
    "                 y_test=y_test,\n",
    "                 y_pred=y_pred,\n",
    "                 variavel_alvo='NUM_NOTA_CH',\n",
    "                 modelo=modelo_lgbm,\n",
    "                 nome_modelo='modelo_lgbm_base_censo_enem',\n",
    "                 descricao_modelo='Modelo LGBMRegressor base Censo e ENEM 2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "680b8f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 54.7139\n",
      "RMSE (treino): 69.3612\n",
      "R2 (treino): 0.3287\n",
      "MAE (teste): 55.1720\n",
      "RMSE (teste): 69.8875\n",
      "R2 (teste): 0.3149\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20970e9",
   "metadata": {},
   "source": [
    "Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52948cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lgbm_bayes = lgbm.LGBMRegressor(random_state=42, \n",
    "                                       early_stopping_rounds=200,\n",
    "                                       max_bin=4095, \n",
    "                                       force_row_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd0f50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (5, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (40, 100),                        # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.005, 0.1, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                  # N√∫mero de √°rvores\n",
    "    'subsample': (0.3, 1.0),                       # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.2, 1.0),                # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1.0, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-5, 1.0, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a9c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_lgbm_bayes,    # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=30,                      # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=5,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b32b2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_bayes, X_eval, y_train_bayes, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train['NUM_NOTA_CH'],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c59f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_set': [(X_eval, y_eval)],               # Conjunto de valida√ß√£o\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],         # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,  # Colunas categ√≥ricas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e886a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 29908\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 106\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 527.881246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4993]\tvalid_0's rmse: 69.7303\tvalid_0's l1: 54.9822\tvalid_0's l2: 4862.32\n"
     ]
    }
   ],
   "source": [
    "# Executar a busca Bayesiana\n",
    "\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_bayes, y_train_bayes, **fit_params)\n",
    "\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20649f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores par√¢metros encontrados:\n",
      "OrderedDict([('colsample_bytree', 0.2663042735572549), ('learning_rate', 0.010597345238569207), ('max_depth', 44), ('n_estimators', 5536), ('num_leaves', 59), ('reg_alpha', 0.27257541504362476), ('reg_lambda', 0.00038430430253506674), ('subsample', 0.334370980765804)])\n",
      "R2:  0.3215118024909344\n",
      "Tempo total de execu√ß√£o: 26952.51 segundos\n"
     ]
    }
   ],
   "source": [
    "# Resultados da busca Bayesiana\n",
    "\n",
    "print(\"Melhores par√¢metros encontrados:\")\n",
    "print(bayes_search.best_params_)\n",
    "print(\"R2: \", bayes_search.best_score_)\n",
    "print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Melhores parametros obtidos entre os modelos otimizados\n",
    "# parametros_best_bayes = {\n",
    "#     \"colsample_bytree\": 0.2663042735572549,\n",
    "#     \"learning_rate\": 0.010597345238569207,\n",
    "#     \"max_depth\": 44,\n",
    "#     \"n_estimators\": 5536,\n",
    "#     \"num_leaves\": 59,\n",
    "#     \"reg_alpha\": 0.27257541504362476,\n",
    "#     \"reg_lambda\": 0.00038430430253506674,\n",
    "#     \"subsample\": 0.334370980765804\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fce45e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 29908\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 106\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 527.881246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4993]\tvalid_0's rmse: 69.7303\tvalid_0's l1: 54.9822\tvalid_0's l2: 4862.32\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.set_params(**bayes_search.best_params_)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.fit(X_train_bayes, \n",
    "                      y_train_bayes, \n",
    "                      eval_set=[(X_eval, y_eval)], \n",
    "                      eval_metric=['r2', 'rmse', 'mae'],\n",
    "                      categorical_feature=categorical_features,\n",
    "                      callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c52e8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_bayes = modelo_lgbm_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "882b3b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/05 06:24:01 WARNING mlflow.models.signature: Failed to infer schema for inputs. Setting schema to `Schema([ColSpec(type=AnyType())]` as default. Note that MLflow doesn't validate data types during inference for AnyType. To see the full traceback, set logging level to DEBUG.\n",
      "Registered model 'modelo_lgbm_bayes_censo_enem' already exists. Creating a new version of this model...\n",
      "2025/06/05 06:24:15 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_bayes_censo_enem, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run crawling-mule-192 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/5323bc201697419092f725769f16d1f8\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_bayes_censo_enem\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'modelo_lgbm_bayes_censo_enem'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_bayes,\n",
    "                    parametros={**modelo_lgbm_bayes.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_bayes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_bayes_censo_enem',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV Censo e ENEM 2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67d1da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 52.9494\n",
      "RMSE (treino): 67.2049\n",
      "R2 (treino): 0.3698\n",
      "MAE (teste): 54.7656\n",
      "RMSE (teste): 69.4478\n",
      "R2 (teste): 0.3235\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_bayes.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_bayes, \"teste\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
