{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d08606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV  # Bayesian optimization: utilizado para optimizar hiperpar√°metros\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import early_stopping  # Early stopping: utilizado para evitar sobreajuste\n",
    "\n",
    "from Funcoes_Comuns import avaliar_modelo, registrar_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5585f",
   "metadata": {},
   "source": [
    "Recuperar base j√° pr√©-processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad41c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\MICRODADOS_ENEM_2023_tratados.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baea006",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc758f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste de tipo para MLflow\n",
    "# Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5c6ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAT_COR_RACA',\n",
       " 'CAT_CO_MUNICIPIO_ESC',\n",
       " 'CAT_CO_UF_ESC',\n",
       " 'CAT_DEPENDENCIA_ADM_ESC',\n",
       " 'CAT_ENSINO',\n",
       " 'CAT_ESCOLA',\n",
       " 'CAT_ESTADO_CIVIL',\n",
       " 'CAT_FAIXA_ETARIA',\n",
       " 'CAT_LINGUA',\n",
       " 'CAT_LOCALIZACAO_ESC',\n",
       " 'CAT_NACIONALIDADE',\n",
       " 'CAT_Q003',\n",
       " 'CAT_Q004',\n",
       " 'CAT_SEXO',\n",
       " 'CAT_SIT_FUNC_ESC']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adacb19",
   "metadata": {},
   "source": [
    "Modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be32573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4244\n",
      "[LightGBM] [Info] Number of data points in the train set: 573256, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 527.936960\n"
     ]
    }
   ],
   "source": [
    "# Treinar modelo LGBMRegressor Base\n",
    "modelo_lgbm = lgbm.LGBMRegressor(n_estimators=1000, \n",
    "                                 learning_rate=0.01, \n",
    "                                 random_state=42,\n",
    "                                 max_bin=4095,\n",
    "                                 force_row_wise=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "modelo_lgbm.fit(X_train, \n",
    "                y_train['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_test, y_test['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features)\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f9a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred = modelo_lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186e0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_base' already exists. Creating a new version of this model...\n",
      "2025/05/21 00:03:43 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_base, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run carefree-fox-690 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/7b06c01669424b8aaa400e109a7ffae5\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_base\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'modelo_lgbm_base'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                 parametros={**modelo_lgbm.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                 X_train=X_train,\n",
    "                 y_train=y_train,\n",
    "                 y_test=y_test,\n",
    "                 y_pred=y_pred,\n",
    "                 variavel_alvo='NUM_NOTA_CH',\n",
    "                 modelo=modelo_lgbm,\n",
    "                 nome_modelo='modelo_lgbm_base',\n",
    "                 descricao_modelo='Modelo LGBMRegressor base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d5ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': None,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0,\n",
       " 'max_bin': 4095,\n",
       " 'force_row_wise': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_lgbm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8846746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.68\n",
      "RMSE (treino): 68.10\n",
      "R2 (treino): 0.35\n",
      "MAE (teste): 55.34\n",
      "RMSE (teste): 70.14\n",
      "R2 (teste): 0.31\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred, \"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75b3412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modelos\\\\modelo_lgbm_base.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvar o modelo otimizado como um arquivo pickle\n",
    "joblib.dump(modelo_lgbm, 'Modelos\\modelo_lgbm_base.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4743201",
   "metadata": {},
   "source": [
    "Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a727da",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lgbm_bayes = lgbm.LGBMRegressor(random_state=42, \n",
    "                                       early_stopping_rounds=200,\n",
    "                                       max_bin=4095, \n",
    "                                       force_row_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac8412f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (5, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (40, 100),                        # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.005, 0.1, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                  # N√∫mero de √°rvores\n",
    "    'subsample': (0.3, 1.0),                       # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.2, 1.0),                # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1.0, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-5, 1.0, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d16ccdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_lgbm_bayes,    # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=30,                      # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=5,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a13433ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_bayes, X_eval, y_train_bayes, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train['NUM_NOTA_CH'],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7078d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_set': [(X_eval, y_eval)],               # Conjunto de valida√ß√£o\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],         # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,  # Colunas categ√≥ricas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63fb8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 4289\n",
      "[LightGBM] [Info] Number of data points in the train set: 487267, number of used features: 40\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 527.961360\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3452]\tvalid_0's rmse: 69.8276\tvalid_0's l1: 55.0658\tvalid_0's l2: 4875.9\n"
     ]
    }
   ],
   "source": [
    "# Executar a busca Bayesiana\n",
    "\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_bayes, y_train_bayes, **fit_params)\n",
    "\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e56395a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores par√¢metros encontrados:\n",
      "OrderedDict([('colsample_bytree', 0.2297823151086026), ('learning_rate', 0.028382425255806625), ('max_depth', 40), ('n_estimators', 5990), ('num_leaves', 40), ('reg_alpha', 0.00596084139054512), ('reg_lambda', 0.8601375912019539), ('subsample', 0.9434176158805601)])\n",
      "R2:  0.32024522308991327\n",
      "Tempo total de execu√ß√£o: 10495.70 segundos\n"
     ]
    }
   ],
   "source": [
    "# Resultados da busca Bayesiana\n",
    "\n",
    "print(\"Melhores par√¢metros encontrados:\")\n",
    "print(bayes_search.best_params_)\n",
    "print(\"R2: \", bayes_search.best_score_)\n",
    "print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72438c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 4289\n",
      "[LightGBM] [Info] Number of data points in the train set: 487267, number of used features: 40\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 527.961360\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3452]\tvalid_0's rmse: 69.8276\tvalid_0's l1: 55.0658\tvalid_0's l2: 4875.9\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.set_params(**bayes_search.best_params_)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.fit(X_train_bayes, \n",
    "                      y_train_bayes, \n",
    "                      eval_set=[(X_eval, y_eval)], \n",
    "                      eval_metric=['r2', 'rmse', 'mae'],\n",
    "                      categorical_feature=categorical_features,\n",
    "                      callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18ed2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_bayes = modelo_lgbm_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17e84c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_bayes' already exists. Creating a new version of this model...\n",
      "2025/05/22 02:23:00 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_bayes, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run big-sheep-798 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/b1086e3116324812b68676927dbbe8df\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_bayes\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'modelo_lgbm_bayes'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_bayes,\n",
    "                    parametros={**modelo_lgbm_bayes.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_bayes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_bayes',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1c47eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.20\n",
      "RMSE (treino): 67.55\n",
      "R2 (treino): 0.36\n",
      "MAE (teste): 54.99\n",
      "RMSE (teste): 69.76\n",
      "R2 (teste): 0.32\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_bayes.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_bayes, \"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c20721a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modelos\\\\modelo_lgbm_bayes.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvar o modelo otimizado como um arquivo pickle\n",
    "joblib.dump(modelo_lgbm_bayes, 'Modelos\\modelo_lgbm_bayes.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
