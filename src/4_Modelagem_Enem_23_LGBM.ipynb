{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38d08606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV  # Bayesian optimization: utilizado para optimizar hiperpar√°metros\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import early_stopping  # Early stopping: utilizado para evitar sobreajuste\n",
    "\n",
    "from Funcoes_Comuns import avaliar_modelo, registrar_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5585f",
   "metadata": {},
   "source": [
    "Recuperar base j√° pr√©-processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bad41c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\Finais\\enem_2023_full.pkl')\n",
    "\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow\n",
    "# Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "# Apenas utilizado nos modelos fianais, BayesSearchCV n√£o utiliza Eval Set j√° possui validacao cruzada interna\n",
    "X_train_final, X_eval, y_train_final, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adacb19",
   "metadata": {},
   "source": [
    "### 1. Modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1be32573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4296\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 527.881246\n"
     ]
    }
   ],
   "source": [
    "# Treinar modelo LGBMRegressor Base\n",
    "modelo_lgbm = lgbm.LGBMRegressor(n_estimators=1000, \n",
    "                                 learning_rate=0.01, \n",
    "                                 random_state=42,\n",
    "                                 max_bin=4095,\n",
    "                                 force_row_wise=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "modelo_lgbm.fit(X_train_final, \n",
    "                y_train_final['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features)\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3f9a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred = modelo_lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186e0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_base' already exists. Creating a new version of this model...\n",
      "2025/06/07 23:02:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_base, version 3\n",
      "Created version '3' of model 'modelo_lgbm_base'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run resilient-pig-154 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/95fb16e000d14a73be8a3e660aa993f5\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_base\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                 parametros={**modelo_lgbm.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                 X_train=X_train,\n",
    "                 y_train=y_train,\n",
    "                 y_test=y_test,\n",
    "                 y_pred=y_pred,\n",
    "                 variavel_alvo='NUM_NOTA_CH',\n",
    "                 modelo=modelo_lgbm,\n",
    "                 nome_modelo='modelo_lgbm_base',\n",
    "                 descricao_modelo='Modelo LGBMRegressor base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8846746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.8265\n",
      "RMSE (treino): 68.2660\n",
      "R2 (treino): 0.3498\n",
      "MAE (teste): 55.2285\n",
      "RMSE (teste): 70.0224\n",
      "R2 (teste): 0.3122\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred, \"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e75b3412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modelos\\\\modelo_lgbm_base.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvar o modelo otimizado como um arquivo pickle\n",
    "joblib.dump(modelo_lgbm, 'Modelos\\modelo_lgbm_base.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4743201",
   "metadata": {},
   "source": [
    "### 2. Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64a727da",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lgbm_bayes = lgbm.LGBMRegressor(random_state=42,\n",
    "                                       max_bin=4095, \n",
    "                                       force_row_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac8412f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (5, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (40, 100),                        # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.005, 0.1, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                  # N√∫mero de √°rvores\n",
    "    'subsample': (0.3, 1.0),                       # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.2, 1.0),                # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1.0, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-5, 1.0, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d16ccdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_lgbm_bayes,    # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=30,                      # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=5,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7078d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categ√≥ricas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63fb8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Executar a busca Bayesiana\u001b[39;00m\n\u001b[0;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mbayes_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNUM_NOTA_CH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Parar o cron√¥metro\u001b[39;00m\n\u001b[0;32m      7\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\skopt\\searchcv.py:542\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[1;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit):\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayesSearchCV doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support a callable refit, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt define an implicit score to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m     )\n\u001b[1;32m--> 542\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\skopt\\searchcv.py:599\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[1;32m--> 599\u001b[0m     optim_result, score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\skopt\\searchcv.py:453\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[1;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# make lists into dictionaries\u001b[39;00m\n\u001b[0;32m    451\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m [point_asdict(search_space, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[1;32m--> 453\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# to get the score name\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2071\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1681\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1681\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1799\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[1;32m-> 1799\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1800\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1803\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1810\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Executar a busca Bayesiana\n",
    "\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train, y_train['NUM_NOTA_CH'], **fit_params)\n",
    "\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56395a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao obter melhores par√¢metros, usando valores calculados anteriormente:\n",
      " {'colsample_bytree': 0.2297823151086026, 'learning_rate': 0.028382425255806625, 'max_depth': 40, 'n_estimators': 5990, 'num_leaves': 40, 'reg_alpha': 0.00596084139054512, 'reg_lambda': 0.8601375912019539, 'subsample': 0.9434176158805601}\n"
     ]
    }
   ],
   "source": [
    "# Melhores par√¢metros encontrados\n",
    "try:\n",
    "    melhores_parametros = bayes_search.best_params_\n",
    "    print(f\"Melhores par√¢metros: {melhores_parametros}\")\n",
    "    print(\"R2: \", bayes_search.best_score_)\n",
    "    print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")\n",
    "except:\n",
    "    melhores_parametros = {\n",
    "        \"colsample_bytree\": 0.2297823151086026,\n",
    "        \"learning_rate\": 0.028382425255806625,\n",
    "        \"max_depth\": 40,\n",
    "        \"n_estimators\": 5990,\n",
    "        \"num_leaves\": 40,\n",
    "        \"reg_alpha\": 0.00596084139054512,\n",
    "        \"reg_lambda\": 0.8601375912019539,\n",
    "        \"subsample\": 0.9434176158805601\n",
    "    }\n",
    "    print(f\"Erro ao obter melhores par√¢metros, usando valores calculados anteriormente:\\n {melhores_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72438c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4296\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 527.881246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2901]\tvalid_0's rmse: 69.8154\tvalid_0's l1: 55.0559\tvalid_0's l2: 4874.18\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.fit(X_train_final,\n",
    "                      y_train_final['NUM_NOTA_CH'],\n",
    "                      eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                      eval_metric=['r2', 'rmse', 'mae'],\n",
    "                      categorical_feature=categorical_features,\n",
    "                      callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_bayes = modelo_lgbm_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e84c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_bayes' already exists. Creating a new version of this model...\n",
      "2025/06/07 23:26:47 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_bayes, version 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run amazing-carp-611 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/40540fd080d74b25b19b1833aee7ca39\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_bayes\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '5' of model 'modelo_lgbm_bayes'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_bayes,\n",
    "                    parametros={**modelo_lgbm_bayes.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_bayes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_bayes',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c47eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.3724\n",
      "RMSE (treino): 67.7531\n",
      "R2 (treino): 0.3595\n",
      "MAE (teste): 54.8517\n",
      "RMSE (teste): 69.5887\n",
      "R2 (teste): 0.3207\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_bayes.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_bayes, \"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20721a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modelos\\\\modelo_lgbm_bayes.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvar o modelo otimizado como um arquivo pickle\n",
    "joblib.dump(modelo_lgbm_bayes, 'Modelos\\modelo_lgbm_bayes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22258",
   "metadata": {},
   "source": [
    "### 3. Buscar melhorar modelo alterando Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac00f3b",
   "metadata": {},
   "source": [
    "#### a. Tentar melhorar removendo todas as vari√°veis sem categorias definidas (\"N√£o informado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b537a1",
   "metadata": {},
   "source": [
    "    TP_ESTADO_CIVIL: 0 (N√£o informado)\n",
    "    TP_COR_RACA: 0 (N√£o declarado)\n",
    "    TP_NACIONALIDADE: 0 (N√£o informado)\n",
    "    TP_ESCOLA: 1 (N√£o Respondeu)\n",
    "    TP_ENSINO: 0 (N√£o informado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47008f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\Finais\\enem_2023_full.pkl')\n",
    "\n",
    "# Remover linhas com valores n√£o explicativos\n",
    "df_enem = df_enem[\n",
    "    (df_enem['CAT_NACIONALIDADE'] != 0) &\n",
    "    (df_enem['CAT_ENSINO'] != 0) &\n",
    "    (df_enem['CAT_COR_RACA'] != 0) &\n",
    "    (df_enem['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (df_enem['CAT_ESCOLA'] != 1)\n",
    "]\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_final, X_eval, y_train_final, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30471290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4237\n",
      "[LightGBM] [Info] Number of data points in the train set: 457203, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 529.413394\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3118]\tvalid_0's rmse: 69.7432\tvalid_0's l1: 54.9119\tvalid_0's l2: 4864.11\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor sem categorias definidas\n",
    "modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas = lgbm.LGBMRegressor(random_state=42,\n",
    "                                                                                max_bin=4095, \n",
    "                                                                                force_row_wise=True)\n",
    "\n",
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.fit(X_train_final, \n",
    "                                                                y_train_final['NUM_NOTA_CH'], \n",
    "                                                                eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                                                                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                                                                categorical_feature=categorical_features,\n",
    "                                                                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c532044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_explicativas = modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas' already exists. Creating a new version of this model...\n",
      "2025/06/07 23:37:31 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run suave-shark-495 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/303d650dedda447b920aaa527264eb0e\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas,\n",
    "                    parametros={**modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_explicativas,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV sem categorias definidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b12cfc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 52.8961\n",
      "RMSE (treino): 67.2000\n",
      "R2 (treino): 0.3653\n",
      "MAE (teste): 54.6748\n",
      "RMSE (teste): 69.5368\n",
      "R2 (teste): 0.3220\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_explicativas, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912a77f",
   "metadata": {},
   "source": [
    "### 2. Tentar melhorar removendo vari√°veis de menor import√¢ncia (import√¢ncia e informa√ß√£o m√∫tua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040510fd",
   "metadata": {},
   "source": [
    "    CAT_SIT_FUNC_ESC\n",
    "    CAT_NACIONALIDADE\n",
    "    CAT_ENSINO\n",
    "    CAT_LOCALIZACAO_ESC\n",
    "    CAT_ESTADO_CIVIL\n",
    "    BIN_Q002_DUMMY_H\n",
    "    NUM_Q017\n",
    "    NUM_Q012\n",
    "    BIN_Q001_DUMMY_H\n",
    "    NUM_Q015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\Finais\\enem_2023_full.pkl')\n",
    "\n",
    "# Remover algumas colunas com MENOR valor explicativo\n",
    "df_enem = df_enem.drop(columns=['CAT_SIT_FUNC_ESC',\n",
    "                                'CAT_NACIONALIDADE',\n",
    "                                'CAT_ENSINO',\n",
    "                                'CAT_LOCALIZACAO_ESC',\n",
    "                                'CAT_ESTADO_CIVIL',\n",
    "                                'BIN_Q002_DUMMY_H',\n",
    "                                'NUM_Q017',\n",
    "                                'NUM_Q012',\n",
    "                                'BIN_Q001_DUMMY_H',\n",
    "                                'NUM_Q015',\n",
    "                                ])\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_final, X_eval, y_train_final, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4258\n",
      "[LightGBM] [Info] Number of data points in the train set: 487214, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 527.881246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1291]\tvalid_0's rmse: 70.0128\tvalid_0's l1: 55.2252\tvalid_0's l2: 4901.79\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor com redu√ß√£o de features\n",
    "modelo_lgbm_reducao_de_features = lgbm.LGBMRegressor(random_state=42,\n",
    "                                                        max_bin=4095, \n",
    "                                                        force_row_wise=True)\n",
    "\n",
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_reducao_de_features.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_reducao_de_features.fit(X_train_final,\n",
    "                                    y_train_final['NUM_NOTA_CH'], \n",
    "                                    eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                                    eval_metric=['r2', 'rmse', 'mae'],\n",
    "                                    categorical_feature=categorical_features,\n",
    "                                    callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8ea254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_var_importantes = modelo_lgbm_reducao_de_features.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8485081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_reducao_de_10_features' already exists. Creating a new version of this model...\n",
      "2025/06/07 23:42:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_reducao_de_10_features, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run amazing-midge-269 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/3dcfbf3840ba41ceb594d915223aba03\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_reducao_de_10_features\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'modelo_lgbm_reducao_de_10_features'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_reducao_de_features,\n",
    "                    parametros={**modelo_lgbm_reducao_de_features.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_var_importantes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redu√ß√£o de 10 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2833d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.9122\n",
      "RMSE (treino): 68.3946\n",
      "R2 (treino): 0.3469\n",
      "MAE (teste): 55.0416\n",
      "RMSE (teste): 69.7828\n",
      "R2 (teste): 0.3169\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_reducao_de_features.predict(X_train), \"treino\")\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_var_importantes, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371aee35",
   "metadata": {},
   "source": [
    "### 3. Aplicar os dois: remover categorias n√£o explicativas + remover colunas menos importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e70acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\Finais\\enem_2023_full.pkl')\n",
    "\n",
    "# Remover linhas com valores n√£o explicativos\n",
    "df_enem = df_enem[\n",
    "    (df_enem['CAT_NACIONALIDADE'] != 0) &\n",
    "    (df_enem['CAT_ENSINO'] != 0) &\n",
    "    (df_enem['CAT_COR_RACA'] != 0) &\n",
    "    (df_enem['CAT_ESTADO_CIVIL'] != 0) &\n",
    "    (df_enem['CAT_ESCOLA'] != 1)\n",
    "]\n",
    "\n",
    "# Remover algumas colunas com MENOR valor explicativo\n",
    "df_enem = df_enem.drop(columns=['CAT_SIT_FUNC_ESC',\n",
    "                                'CAT_NACIONALIDADE',\n",
    "                                'CAT_ENSINO',\n",
    "                                'CAT_LOCALIZACAO_ESC',\n",
    "                                'CAT_ESTADO_CIVIL',\n",
    "                                'BIN_Q002_DUMMY_H',\n",
    "                                'NUM_Q017',\n",
    "                                'NUM_Q012',\n",
    "                                'BIN_Q001_DUMMY_H',\n",
    "                                'NUM_Q015',\n",
    "                                ])\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_final, X_eval, y_train_final, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4201\n",
      "[LightGBM] [Info] Number of data points in the train set: 457203, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 529.413394\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2577]\tvalid_0's rmse: 69.9135\tvalid_0's l1: 55.0636\tvalid_0's l2: 4887.9\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo LGBMRegressor com redu√ß√£o de features e categorias menos explicativas\n",
    "modelo_lgbm_reducao_de_features_categorias_nao_explicativas = lgbm.LGBMRegressor(random_state=42,\n",
    "                                                                                    max_bin=4095, \n",
    "                                                                                    force_row_wise=True)\n",
    "\n",
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_reducao_de_features_categorias_nao_explicativas.set_params(**melhores_parametros)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_reducao_de_features_categorias_nao_explicativas.fit(X_train_final,\n",
    "                                                                y_train_final['NUM_NOTA_CH'], \n",
    "                                                                eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                                                                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                                                                categorical_feature=categorical_features,\n",
    "                                                                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5918d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_var_feat_categ = modelo_lgbm_reducao_de_features_categorias_nao_explicativas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_reducao_de_features_categorias_nao_explicativas,\n",
    "                    parametros={**modelo_lgbm_reducao_de_features_categorias_nao_explicativas.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=modelo_lgbm_reducao_de_features_categorias_nao_explicativas.predict(X_test),\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features_categorias_nao_explicativas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redu√ß√£o de 10 features e categorias n√£o explicativas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4b89abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.3024\n",
      "RMSE (treino): 67.6805\n",
      "R2 (treino): 0.3562\n",
      "MAE (teste): 54.7989\n",
      "RMSE (teste): 69.6746\n",
      "R2 (teste): 0.3193\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_reducao_de_features_categorias_nao_explicativas.predict(X_train), \"treino\")\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_var_feat_categ, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf74892",
   "metadata": {},
   "source": [
    "Conclus√£o\n",
    "\n",
    "- Remo√ß√£o de categorias n√£o explicativas\n",
    "    - mantendo fixo os par√¢metros do modelo e removendo apenas as linhas com categorias n√£o explicativas houve uma melhora do modelo\n",
    "    - R¬≤ de 0.3207 para 0.3220\n",
    "\n",
    "- Remo√ß√£o das 10 vari√°veis menos explicativas\n",
    "    - mantendo fixo os par√¢metros do modelo e removendo apenas as colunas com menor import√¢ncia para o alvo houve uma piora do modelo\n",
    "    - Removendo 10: R¬≤ de 0.3207 para 0.3169\n",
    "\n",
    "- Aplicando as duas t√©nicas\n",
    "    - Redu√ß√£o R¬≤ de 0.3207 para 0.3193\n",
    "\n",
    "Padr√£o se mant√©m para outras m√©tricas RMSE e MAE\n",
    "\n",
    "√â interessante remover todas as categorias n√£o explicativas, mostra uma melhora no modelo, mas manter todas as vari√°veis dispon√≠veis, mesmo se menos importantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
