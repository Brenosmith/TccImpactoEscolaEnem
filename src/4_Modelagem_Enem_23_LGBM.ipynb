{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38d08606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV  # Bayesian optimization: utilizado para optimizar hiperpar√°metros\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import early_stopping  # Early stopping: utilizado para evitar sobreajuste\n",
    "\n",
    "from Funcoes_Comuns import avaliar_modelo, registrar_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5585f",
   "metadata": {},
   "source": [
    "Recuperar base j√° pr√©-processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad41c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\MICRODADOS_ENEM_2023_tratados.pkl')\n",
    "\n",
    "# Base removendo as categorias que n√£o s√£o explicativas + as 10 vari√°veis menos importantes \n",
    "# df_enem = pd.read_pickle('Bases\\MICRODADOS_ENEM_2023_tratados_variaveis_importante_explicativos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9baea006",
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffc758f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste de tipo para MLflow\n",
    "# Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a5c6ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAT_COR_RACA',\n",
       " 'CAT_CO_MUNICIPIO_ESC',\n",
       " 'CAT_CO_UF_ESC',\n",
       " 'CAT_DEPENDENCIA_ADM_ESC',\n",
       " 'CAT_ENSINO',\n",
       " 'CAT_ESCOLA',\n",
       " 'CAT_ESTADO_CIVIL',\n",
       " 'CAT_FAIXA_ETARIA',\n",
       " 'CAT_LINGUA',\n",
       " 'CAT_LOCALIZACAO_ESC',\n",
       " 'CAT_NACIONALIDADE',\n",
       " 'CAT_Q003',\n",
       " 'CAT_Q004',\n",
       " 'CAT_SEXO',\n",
       " 'CAT_SIT_FUNC_ESC']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adacb19",
   "metadata": {},
   "source": [
    "Modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be32573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 4244\n",
      "[LightGBM] [Info] Number of data points in the train set: 573256, number of used features: 40\n",
      "[LightGBM] [Info] Start training from score 527.936960\n"
     ]
    }
   ],
   "source": [
    "# Treinar modelo LGBMRegressor Base\n",
    "modelo_lgbm = lgbm.LGBMRegressor(n_estimators=1000, \n",
    "                                 learning_rate=0.01, \n",
    "                                 random_state=42,\n",
    "                                 max_bin=4095,\n",
    "                                 force_row_wise=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "modelo_lgbm.fit(X_train, \n",
    "                y_train['NUM_NOTA_CH'], \n",
    "                eval_set=[(X_test, y_test['NUM_NOTA_CH'])], \n",
    "                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                categorical_feature=categorical_features)\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f9a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred = modelo_lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186e0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_base' already exists. Creating a new version of this model...\n",
      "2025/05/21 00:03:43 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_base, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run carefree-fox-690 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/7b06c01669424b8aaa400e109a7ffae5\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_base\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'modelo_lgbm_base'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                 parametros={**modelo_lgbm.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                 X_train=X_train,\n",
    "                 y_train=y_train,\n",
    "                 y_test=y_test,\n",
    "                 y_pred=y_pred,\n",
    "                 variavel_alvo='NUM_NOTA_CH',\n",
    "                 modelo=modelo_lgbm,\n",
    "                 nome_modelo='modelo_lgbm_base',\n",
    "                 descricao_modelo='Modelo LGBMRegressor base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d5ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.01,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': None,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0,\n",
       " 'max_bin': 4095,\n",
       " 'force_row_wise': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_lgbm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8846746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.68\n",
      "RMSE (treino): 68.10\n",
      "R2 (treino): 0.35\n",
      "MAE (teste): 55.34\n",
      "RMSE (teste): 70.14\n",
      "R2 (teste): 0.31\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred, \"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75b3412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modelos\\\\modelo_lgbm_base.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvar o modelo otimizado como um arquivo pickle\n",
    "joblib.dump(modelo_lgbm, 'Modelos\\modelo_lgbm_base.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4743201",
   "metadata": {},
   "source": [
    "Bayes Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a727da",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lgbm_bayes = lgbm.LGBMRegressor(random_state=42,\n",
    "                                       max_bin=4095, \n",
    "                                       force_row_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac8412f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o do espa√ßo de busca para otimiza√ß√£o bayesiana\n",
    "param_grid = {\n",
    "    'num_leaves': (5, 60),                         # N√∫mero de folhas na √°rvore de decis√£o\n",
    "    'max_depth': (40, 100),                        # Profundidade m√°xima da √°rvore\n",
    "    'learning_rate': (0.005, 0.1, 'log-uniform'),  # Taxa de aprendizado\n",
    "    'n_estimators': (5000, 6000),                  # N√∫mero de √°rvores\n",
    "    'subsample': (0.3, 1.0),                       # Propor√ß√£o de amostras usadas em cada √°rvore\n",
    "    'colsample_bytree': (0.2, 1.0),                # Fra√ß√£o de colunas a serem usadas por √°rvore\n",
    "    'reg_alpha': (1e-3, 1.0, 'log-uniform'),       # Regulariza√ß√£o L1\n",
    "    'reg_lambda': (1e-5, 1.0, 'log-uniform'),      # Regulariza√ß√£o L2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d16ccdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar a busca Bayesiana usando BayesSearchCV\n",
    "\n",
    "# Criando o otimizador Bayesiano\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=modelo_lgbm_bayes,    # Modelo a ser otimizado\n",
    "    search_spaces=param_grid,       # Espa√ßo de busca definido acima\n",
    "    scoring='r2',                   # Crit√©rio de sele√ß√£o\n",
    "    n_iter=30,                      # N√∫mero de avalia√ß√µes do modelo\n",
    "    cv=5,                           # Valida√ß√£o cruzada\n",
    "    random_state=42,                # Semente para reprodutibilidade\n",
    "    n_jobs=-1,                      # Paraleliza√ß√£o total dos c√°lculos\n",
    "    verbose=1                       # 0 = sem mensagens, 1 = mensagens de progresso, 2 = mensagens detalhadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a13433ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train_bayes, X_eval, y_train_bayes, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train['NUM_NOTA_CH'],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_set': [(X_eval, y_eval)],                     # Conjunto de valida√ß√£o\n",
    "    'eval_metric': ['r2', 'rmse', 'mae'],               # M√©tricas a serem avaliadas\n",
    "    'categorical_feature': categorical_features,        # Colunas categ√≥ricas\n",
    "    'callbacks': [early_stopping(stopping_rounds=200)]  # Parar se n√£o houver melhoria\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63fb8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 457253, number of used features: 30\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 529.403061\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5030]\tvalid_0's rmse: 70.0743\tvalid_0's l1: 55.23\tvalid_0's l2: 4910.41\n"
     ]
    }
   ],
   "source": [
    "# Executar a busca Bayesiana\n",
    "\n",
    "start_time = time.time()\n",
    "bayes_search.fit(X_train_bayes, y_train_bayes, **fit_params)\n",
    "\n",
    "# Parar o cron√¥metro\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56395a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados da busca Bayesiana\n",
    "\n",
    "print(\"Melhores par√¢metros encontrados:\")\n",
    "print(bayes_search.best_params_)\n",
    "print(\"R2: \", bayes_search.best_score_)\n",
    "print(f\"Tempo total de execu√ß√£o: {elapsed_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74b910a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melhores parametros obtidos entre os modelos otimizados\n",
    "parametros_best_bayes = {\n",
    "    \"colsample_bytree\": 0.2297823151086026,\n",
    "    \"learning_rate\": 0.028382425255806625,\n",
    "    \"max_depth\": 40,\n",
    "    \"n_estimators\": 5990,\n",
    "    \"num_leaves\": 40,\n",
    "    \"reg_alpha\": 0.00596084139054512,\n",
    "    \"reg_lambda\": 0.8601375912019539,\n",
    "    \"subsample\": 0.9434176158805601\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "72438c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 4289\n",
      "[LightGBM] [Info] Number of data points in the train set: 487267, number of used features: 40\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 527.961360\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3452]\tvalid_0's rmse: 69.8276\tvalid_0's l1: 55.0658\tvalid_0's l2: 4875.9\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.set_params(**bayes_search.best_params_)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_bayes.fit(X_train_bayes, \n",
    "                      y_train_bayes, \n",
    "                      eval_set=[(X_eval, y_eval)], \n",
    "                      eval_metric=['r2', 'rmse', 'mae'],\n",
    "                      categorical_feature=categorical_features,\n",
    "                      callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18ed2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_bayes = modelo_lgbm_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17e84c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_bayes' already exists. Creating a new version of this model...\n",
      "2025/06/02 22:30:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_bayes, version 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run bittersweet-slug-181 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/5b3c10e72ca74e018ca4b6786632bcd3\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_bayes\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '4' of model 'modelo_lgbm_bayes'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_bayes,\n",
    "                    parametros={**modelo_lgbm_bayes.get_params(), \"amostra\": X_train.shape[0], \"tempo\": tempo_treino},\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_bayes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_bayes',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1c47eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 53.1950\n",
      "RMSE (treino): 67.5541\n",
      "R2 (treino): 0.3628\n",
      "MAE (teste): 54.9856\n",
      "RMSE (teste): 69.7555\n",
      "R2 (teste): 0.3192\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_bayes.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_bayes, \"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c20721a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modelos\\\\modelo_lgbm_bayes.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvar o modelo otimizado como um arquivo pickle\n",
    "joblib.dump(modelo_lgbm_bayes, 'Modelos\\modelo_lgbm_bayes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f22258",
   "metadata": {},
   "source": [
    "### Buscar melhorar modelo alterando Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melhores parametros obtidos entre os modelos otimizados\n",
    "parametros_best_bayes = {\n",
    "    \"colsample_bytree\": 0.2297823151086026,\n",
    "    \"learning_rate\": 0.028382425255806625,\n",
    "    \"max_depth\": 40,\n",
    "    \"n_estimators\": 5990,\n",
    "    \"num_leaves\": 40,\n",
    "    \"reg_alpha\": 0.00596084139054512,\n",
    "    \"reg_lambda\": 0.8601375912019539,\n",
    "    \"subsample\": 0.9434176158805601\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac00f3b",
   "metadata": {},
   "source": [
    "### 1. Tentar melhorar removendo todas as vari√°veis sem categorias definidas (\"N√£o informado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b537a1",
   "metadata": {},
   "source": [
    "    TP_ESTADO_CIVIL: 0 (N√£o informado)\n",
    "    TP_COR_RACA: 0 (N√£o declarado)\n",
    "    TP_NACIONALIDADE: 0 (N√£o informado)\n",
    "    TP_ESCOLA: 1 (N√£o Respondeu)\n",
    "    TP_ENSINO: 0 (N√£o informado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47008f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\MICRODADOS_ENEM_2023_tratados_explicativos.pkl')\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas = lgbm.LGBMRegressor(random_state=42,\n",
    "                                                                                max_bin=4095, \n",
    "                                                                                force_row_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30471290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 4229\n",
      "[LightGBM] [Info] Number of data points in the train set: 457253, number of used features: 40\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 529.403061\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2697]\tvalid_0's rmse: 69.8743\tvalid_0's l1: 55.0455\tvalid_0's l2: 4882.41\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.set_params(**parametros_best_bayes)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.fit(X_train, \n",
    "                                                                y_train['NUM_NOTA_CH'], \n",
    "                                                                eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                                                                eval_metric=['r2', 'rmse', 'mae'],\n",
    "                                                                categorical_feature=categorical_features,\n",
    "                                                                callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c532044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_explicativas = modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f8a9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas' already exists. Creating a new version of this model...\n",
      "2025/06/02 18:22:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run merciful-goat-238 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/70017590ea3a4de5862352707d4fe3ad\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas,\n",
    "                    parametros={\n",
    "                                **modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.get_params(), \n",
    "                                \"amostra\": X_train.shape[0], \n",
    "                                \"tempo\": tempo_treino\n",
    "                                },\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_explicativas,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV sem categorias definidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12cfc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 52.9660\n",
      "RMSE (treino): 67.3000\n",
      "R2 (treino): 0.3630\n",
      "MAE (teste): 54.8429\n",
      "RMSE (teste): 69.6780\n",
      "R2 (teste): 0.3191\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_remocao_de_variaveis_sem_categorias_definidas.predict(X_train), \"treino\")\n",
    "\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_explicativas, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912a77f",
   "metadata": {},
   "source": [
    "### 2. Tentar melhorar removendo vari√°veis de menor import√¢ncia (import√¢ncia e informa√ß√£o m√∫tua)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040510fd",
   "metadata": {},
   "source": [
    "    CAT_SIT_FUNC_ESC\n",
    "    CAT_NACIONALIDADE\n",
    "    CAT_ENSINO\n",
    "    CAT_LOCALIZACAO_ESC\n",
    "    CAT_ESTADO_CIVIL\n",
    "    BIN_Q002_DUMMY_H\n",
    "    NUM_Q017\n",
    "    NUM_Q012\n",
    "    BIN_Q001_DUMMY_H\n",
    "    NUM_Q015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2cd468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter dados\n",
    "df_enem = pd.read_pickle('Bases\\MICRODADOS_ENEM_2023_tratados_variaveis_importantes.pkl')\n",
    "\n",
    "#Variaveis alvo\n",
    "variaveis_alvo = ['NUM_NOTA_MT', 'NUM_NOTA_LC', 'NUM_NOTA_CN', 'NUM_NOTA_CH', 'NUM_NOTA_REDACAO']\n",
    "\n",
    "# separar em treino e teste\n",
    "X = df_enem.drop(columns=variaveis_alvo)\n",
    "y = df_enem[variaveis_alvo]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ajuste de tipo para MLflow -> Converter colunas inteiras para float\n",
    "X_train = X_train.astype({col: 'float' for col in X_train.select_dtypes('int').columns})\n",
    "X_test = X_test.astype({col: 'float' for col in X_test.select_dtypes('int').columns})\n",
    "\n",
    "# Obter colunas categ√≥ricas\n",
    "categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Criar Eval Set para valida√ß√£o cruzada (15% do conjunto de treino)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c57a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lgbm_reducao_de_features = lgbm.LGBMRegressor(random_state=42,\n",
    "                                                        max_bin=4095, \n",
    "                                                        force_row_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a9d39f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Total Bins 4229\n",
      "[LightGBM] [Info] Number of data points in the train set: 457253, number of used features: 40\n",
      "[LightGBM] [Warning] early_stopping_round is set=200, early_stopping_rounds=200 will be ignored. Current value: early_stopping_round=200\n",
      "[LightGBM] [Info] Start training from score 529.403061\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2697]\tvalid_0's rmse: 69.8743\tvalid_0's l1: 55.0455\tvalid_0's l2: 4882.41\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_reducao_de_features.set_params(**parametros_best_bayes)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo com os melhores par√¢metros encontrados\n",
    "modelo_lgbm_reducao_de_features.fit(X_train,\n",
    "                                    y_train['NUM_NOTA_CH'], \n",
    "                                    eval_set=[(X_eval, y_eval['NUM_NOTA_CH'])], \n",
    "                                    eval_metric=['r2', 'rmse', 'mae'],\n",
    "                                    categorical_feature=categorical_features,\n",
    "                                    callbacks=[early_stopping(stopping_rounds=200)])\n",
    "\n",
    "tempo_treino = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ea254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes\n",
    "y_pred_var_importantes = modelo_lgbm_reducao_de_features.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8485081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Armazenamento\\MBA\\TCC\\TccMbaUspEsalq\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'modelo_lgbm_reducao_de_10_features'.\n",
      "2025/06/02 18:36:41 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: modelo_lgbm_reducao_de_10_features, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run ambitious-tern-384 at: http://127.0.0.1:9080/#/experiments/957135083854196683/runs/06b5ee84eed94a2b809288e3b4b2054e\n",
      "üß™ View experiment at: http://127.0.0.1:9080/#/experiments/957135083854196683\n",
      "Modelo registrado com sucesso no MLflow: modelo_lgbm_reducao_de_10_features\n",
      "Rastreamento do MLflow finalizado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'modelo_lgbm_reducao_de_10_features'.\n"
     ]
    }
   ],
   "source": [
    "nome_experimento = 'Notas CH ENEM 2023'\n",
    "\n",
    "# Avaliar o modelo\n",
    "registrar_modelo(experimento=nome_experimento,\n",
    "                    modelo=modelo_lgbm_reducao_de_features,\n",
    "                    parametros={\n",
    "                                **modelo_lgbm_reducao_de_features.get_params(), \n",
    "                                \"amostra\": X_train.shape[0], \n",
    "                                \"tempo\": tempo_treino\n",
    "                                },\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred_var_importantes,\n",
    "                    variavel_alvo='NUM_NOTA_CH',\n",
    "                    nome_modelo='modelo_lgbm_reducao_de_10_features',\n",
    "                    descricao_modelo='Modelo LGBMRegressor otimizado com BayesSearchCV com redu√ß√£o de 10 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2833d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (treino): 52.9660\n",
      "RMSE (treino): 67.3000\n",
      "R2 (treino): 0.3630\n",
      "MAE (teste): 54.8429\n",
      "RMSE (teste): 69.6780\n",
      "R2 (teste): 0.3191\n"
     ]
    }
   ],
   "source": [
    "# Avalia√ß√£o grupo treino\n",
    "avaliar_modelo(y_train['NUM_NOTA_CH'], modelo_lgbm_reducao_de_features.predict(X_train), \"treino\")\n",
    "# Avalia√ß√£o grupo teste\n",
    "avaliar_modelo(y_test['NUM_NOTA_CH'], y_pred_var_importantes, \"teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf74892",
   "metadata": {},
   "source": [
    "Conclus√£o\n",
    "\n",
    "- Remo√ß√£o de categorias n√£o explicativas\n",
    "    - mantendo fixo os par√¢metros do modelo e removendo apenas as linhas com categorias n√£o explicativas houve uma piora do modelo\n",
    "    - R¬≤ de 0.3191 para 0.31908\n",
    "\n",
    "- Remo√ß√£o das piores 5/10 vari√°veis menos explicativas\n",
    "    - mantendo fixo os par√¢metros do modelo e removendo apenas as colunas com menor import√¢ncia para o alvo houve uma piora do modelo\n",
    "    - Removendo 05: R¬≤ de 0.3191 para 0.31908\n",
    "    - Removendo 10: R¬≤ de 0.3191 para 0.31908\n",
    "\n",
    "- Aplicando as duas t√©nicas\n",
    "    - Redu√ß√£o R¬≤ de 0.3191 para 0.3144\n",
    "\n",
    "Padr√£o se mant√©m para outras m√©tricas RMSE e MAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
